import os
os.environ["OLLAMA_NUM_GPU"] = "0"

from langchain.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# Dossier o√π Chroma va stocker les embeddings
DB_DIR = "chroma"  # simplifi√© pour correspondre √† ton app.py

# ===============================
# üß† SYSTEM PROMPT : contexte
# ===============================
SYSTEM_PROMPT = """Tu es un assistant IA qui r√©pond UNIQUEMENT avec le contexte fourni.
- Si l'information n'est pas pr√©sente dans le contexte, dis clairement que tu ne l'as pas.
- R√©ponds en fran√ßais, de mani√®re claire et structur√©e.
- Si possible, cite les sources √† la fin.
--------------------
Contexte:
{context}

Question: {question}

R√©ponse:"""

# ===============================
# üîé R√âCUP√âRATION (RETRIEVER)
# ===============================
def make_retriever(db_dir=DB_DIR, k=3):
    """
    Cr√©e un retriever bas√© sur les embeddings Ollama (nomic-embed-text).
    """
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma(persist_directory=db_dir, embedding_function=embeddings)
    return vectordb.as_retriever(search_kwargs={"k": k})

# ===============================
# üîó CHA√éNE PRINCIPALE RAG
# ===============================
def make_chain(db_dir=DB_DIR):
    """
    Construit la cha√Æne RAG compl√®te :
    1. R√©cup√©ration du contexte via embeddings.
    2. G√©n√©ration de r√©ponse avec mod√®le Ollama (local).
    """
    retriever = make_retriever(db_dir=db_dir, k=3)
    prompt = ChatPromptTemplate.from_template(SYSTEM_PROMPT)

    # S√©lection du mod√®le selon ce qui est dispo
    # (llama3.2:1b recommand√©, sinon phi3:mini)
    try:
        llm = ChatOllama(model="llama3.2:1b", temperature=0.2)
    except Exception:
        llm = ChatOllama(model="phi3:mini", temperature=0.2)

    def format_docs(docs):
        out = []
        for d in docs:
            meta = d.metadata.get("source", "source inconnue")
            out.append(f"[{meta}] {d.page_content}")
        return "\n\n".join(out)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain

# ===============================
# üìä STATISTIQUES D‚ÄôINDEX
# ===============================
def get_index_stats(db_dir="chroma"):
    """
    Retourne le nombre de collections et de chunks index√©s dans Chroma.
    """
    import chromadb
    client = chromadb.PersistentClient(db_dir)
    collections = client.list_collections()
    total_chunks = 0
    for col in collections:
        try:
            total_chunks += len(col.get()['ids'])
        except Exception:
            pass
    return {"collections": len(collections), "chunks": total_chunks}
